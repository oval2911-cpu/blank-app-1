# -*- coding: utf-8 -*-
import streamlit as st

"""
st.title("ðŸŽˆ My new app")
st.write(
    "I now changed somethign"
)"""

"""Framingham Imputed

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19KO2YSUR_sficFbH41WAOjwoGoPPO_b6

#Code for the Framingham Dataset Analysis
---
Group1: Alisa Ovsiannikova i6365923, Tom Einhaus i6339207

*MAI3002 Introduction to Programming*

*Faculty of Health, Medicine, and Life Sciences*

*Maastricht University*

October 28th, 2025

##Contents: **- put links here to scroll to**
###Phase 1
1. Introduction
2. Data preparation
3. Data exploration and cleaning
4. Data description and visualization

###Phase 2
1. Introduction
2. Data preparation
3. Data exploration and cleaning
4. Data description and visualization
5. Data analysis

# Introduction

##Research questions brainstorm:
1. Do participants taking antihypertensive medication have a lower incidence of cardiovascular events (CVD, MI, ANYCHD, stroke) compared to those not taking such medication, or does medication use simply indicate a higher baseline disease burden?
2. What are the demographic and clinical characteristics (age, sex, BMI, cholesterol,  systolic & diastolic BP) of participants who use antihypertensive medication compared to those who do not?
3. Is there a relationship between smoking status and resting heart rate among participants?
4. Among participants with major cardiovascular risk factors (hypertension, smoking, diabetes), how long does it take on average for a stroke (STROKE) or myocardial infarction (HOSPMI) to occur during the follow-up period?
5. What are the demographic and clinical characteristics (age, sex, BMI, cholesterol, systolic & diastolic BP) of participants who developed stroke compared to those who did not?
6. Is there a relationship between BMI and the incidence of myocardial infarction?
7. Are smokers more likely to experience cardiovascular events (CVD, ANYCHD, stroke) compared to non-smokers?
8. Are there differences between men and women in cardiovascular risk profiles (cholesterol, blood pressure, smoking) and event occurrence (CVD, MI, stroke)?
9. What are the demographic and clinical characteristics (age, sex, BMI, cholesterol, diabetes, smoking status, systolic & diastolic BP) of participants who take antihypertensive medication compared to those who do not?
10. What is the occurrence of major cardiovascular events (stroke, myocardial infarction, or coronary heart disease, other?) in participants taking antihypertensive medication compared to those not using it?
11. What other lifestyle / health change other than medication (e.g lowering BMI, quitting smoking) has the best influence on lowering risk for cardiovascular events (stroke, Myocardial infarction)?

# Data preparation

##Load dataset and do some descriptive statistics on the full dataset:
"""

#import libraries
import pandas as pd

#load dataset
data_heart = pd.read_csv('https://raw.githubusercontent.com/LUCE-Blockchain/Databases-for-teaching/refs/heads/main/Framingham%20Dataset.csv')

#see the dataset
data_heart

"""--> each row represents one exam, so there are more rows than number of patients (RANDIDs repeat)

--> can already expect some missing values
"""

#see shape (n of rows and columns) of the dataset
data_heart.shape

#descriptive statistics on full dataset
data_heart.describe()

#look at all column names
data_heart.columns

#summarize information on missing values per feature
data_heart.isna().sum()

"""--> Missing values have to be handled (after we select the subset we will work on)"""

data_heart.duplicated().sum()

"""--> No duplicates to handle

##Identify main research question:
**Main RQ & subquestions:**

Do individuals taking antihypertensive medication differ in their cardiovascular risk profiles and outcomes compared to those not taking medication?
1. Among participants at baseline, how do demographics and clinical characteristics (age, sex, BMI, cholesterol, diabetes, smoking status, systolic & diastolic BP) differ between those who eventually took antihypertensive medication at any of the three follow-ups and those who did not?
2. How does the occurrence of major cardiovascular events differ between non-hypertensive participants and hypertensive participants, and among the latter, between those treated with antihypertensive medication and those untreated across the three follow-up periods?
3. What other lifestyle / health change other than medication (e.g lowering BMI, quitting smoking) has the best influence on lowering risk for cardiovascular events (stroke, myocardial infarction)?

##Select rows and columns relevant to the research question:
"""

#We want to exclude the following columns (number in brackets is index):
#CIGPDAY (7), GLUCOSE (12), educ (13), PREV... (14-18), TIME (19), DEATH (23), ANGINA (24), HOSPMI (25), MI_FCHD (26), TIME..(31-38)
data_heart_subset = data_heart.drop(columns = ['educ', 'DEATH', 'ANGINA', 'HOSPMI', 'MI_FCHD'] #drop these columns
                                    + [col for col in data_heart.columns if col.startswith('PREV')]) #drop columns that start with PREV... when iterating over all columns
#                                    + [col for col in data_heart.columns if col.startswith('TIME')]) #drop columns that start with TIME... when iterating over all columns

#rename subset to make it easier to call it
dhs = data_heart_subset

#check all columns of the created subset
dhs.columns

"""--> it is correct"""

#see the subset
dhs.head(100)

#check the shape of the subset
dhs.shape

"""#Data exploration and cleaning

##Data exploration (distributions and descriptive statistics)
"""

#plot distributions of dataset
import matplotlib.pyplot as plt

dhs = dhs.loc[dhs['PERIOD']==1]

fig, ax = plt.subplots(figsize=(15, 10))

selectedVariable = st.selectbox("Select variable to plot:", dhs.select_dtypes(include='number').columns)
ax.hist(dhs[selectedVariable], bins=30, alpha=0.4, edgecolor='black', label=selectedVariable)

ax.legend()
fig.suptitle(f'Distribution of {selectedVariable} variable', fontsize=20)
fig.tight_layout()

st.pyplot(fig)


"""--> SYSBP, heartrate seem to be right skwewed.

--> Although TOTCHOL, BMI, HDLC and LDLC seem to be also very slightly right skewed, since there are around 12k values and mean and median are close to each other, assuming symmetry is acceptable.

--> Other look normally distributed.

--> Interestingly sex was encoded with 1 and 2 --> be careful

--> Age has some dips?
"""

#plot distributions of variables that are relevant to describe population characteristics at baseline
import matplotlib.pyplot as plt

dhs.loc[dhs['PERIOD']==1].hist(figsize=(15, 10), bins=30, edgecolor='black')
plt.suptitle('Distributions of all numeric variables (continuous and encoded categorical for patients at baseline (period 1))', fontsize=20)
plt.tight_layout()
plt.show()

#show descriptive statistics for unique patients at baseline

"""##Outlier detection and handling"""

#allocate all categorical and numerical variables to corresponding separate variables
categorical = ['SEX', 'CURSMOKE', 'DIABETES', 'BPMEDS',
                    'ANYCHD', 'STROKE', 'CVD', 'HYPERTEN', 'PERIOD']

numerical = ['TOTCHOL', 'AGE', 'SYSBP', 'DIABP', 'BMI', 'HEARTRTE', 'HDLC', 'LDLC']

#create 2 dataframes with different types of variables
num_df = dhs[numerical]
cat_df = dhs[categorical]

import seaborn as sns

#plot boxplots for numerical data to see outliers and dstributions
sns.boxplot(data=dhs[numerical], orient='h')
plt.show()

"""--> most of the outliers are on the right hand side due to skewness"""

#check for ranges of the heart rate and LCDC
print("Highest heart rate:", dhs['HEARTRTE'].max())
print("Lowest heart rate:", dhs['HEARTRTE'].min())

print("Highest LDLC:", dhs['LDLC'].max())
print("Lowest LDLC:", dhs['LDLC'].min())

"""--> 1 heartrate is 220 but stiiiilll technically possible. In 12k data can keep."""

#check for heartrate: how many values are below the "low" value of 60 and higher than "high" values of 120
low = (dhs['HEARTRTE'] < 60).sum()
high = (dhs['HEARTRTE'] > 120).sum()
print("Low (<40):", low, "High (>180):", high)

#check if there are many values outside whiskers (1.5*IQR) - outlier detection
Q1 = num_df.quantile(0.25)
Q3 = num_df.quantile(0.75)
IQR = Q3 - Q1
outlier_mask = (num_df < (Q1 - 1.5 * IQR)) | (num_df > (Q3 + 1.5 * IQR))
outlier_counts = outlier_mask.sum().sort_values(ascending=False)
print(outlier_counts)

#check if there are many values above z score of 3 (another way of detecting outliers)
from scipy.stats import zscore
z_scores = num_df.apply(zscore)
outliers_z = (abs(z_scores) > 3).sum().sort_values(ascending=False)
print(outliers_z)

#How much influence do these outliers have on mean and std when being included or disregarded?

#without outliers
clean_df = num_df[~outlier_mask.any(axis=1)]

#print mean and std for with outliers vs without outliers and put in pd dataframe
compare = pd.DataFrame({
    'Mean (all)': num_df.mean(),
    'Mean (no outliers)': clean_df.mean(),
    'Std (all)': num_df.std(),
    'Std (no outliers)': clean_df.std()
})
print(compare.round(2))

"""TOTCHOL has some influence of outliers. Are they maybe correlated with the group eg SEX? Then these outliers may be valid still."""

sns.violinplot(data=dhs, x='SEX', y='TOTCHOL', palette='Set2')
plt.title('Total Cholesterol by Sex')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt


fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(14,10))
axes = axes.flatten()

for ax, col in zip(axes, num_df.columns):
    sns.histplot(num_df[col], kde=True, ax=ax, color='teal', bins=30)
    ax.set_title(col)

# hide unused subplots
for ax in axes[len(num_df.columns):]:
    ax.set_visible(False)

plt.tight_layout()
plt.show()

from scipy import stats
import matplotlib.pyplot as plt
import math

n_cols = 3
n_rows = 3

fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows * 4))
axes = axes.flatten()

for ax, col in zip(axes, num_df.columns):
    stats.probplot(num_df[col].dropna(), dist="norm", plot=ax)
    ax.set_title(col)

# same as above
for ax in axes[len(num_df.columns):]:
    ax.set_visible(False)

plt.tight_layout()
plt.show()

"""--> Looks reasonable normally distributed.

--> TOTCHOL will be printed later again to show distribution before and after imputation.

##Missing data handling
"""

#check number of missing values in each column
data_heart_subset.isna().sum()

# total number of rows
total_rows = len(dhs)

# number of missing values and percentage for HDL and LDL
missing_counts = dhs[['HDLC', 'LDLC', 'TOTCHOL', 'BPMEDS', 'BMI', 'HEARTRTE']].isna().sum()

for col in ['HDLC', 'LDLC', 'TOTCHOL', 'BPMEDS', 'BMI', 'HEARTRTE']:
    print(col, ":", missing_counts[col], "missing out of", total_rows,
          "(", round(missing_counts[col] / total_rows * 100, 2), "% )")

import matplotlib.pyplot as plt
corr_hdlc = dhs.corr(numeric_only=True)['HDLC'].sort_values(ascending=False)

corr_ldlc = dhs.corr(numeric_only=True)['LDLC'].sort_values(ascending=False)

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# bar plot for HDLC
axes[0].barh(corr_hdlc.index, corr_hdlc.values, color='skyblue')
axes[0].set_title("Correlation of HDLC with Other Variables")
axes[0].set_xlabel("Correlation coefficient (r)")
axes[0].invert_yaxis()  # so the highest correlation is on top

# bar plot for LDLC
axes[1].barh(corr_ldlc.index, corr_ldlc.values, color='salmon')
axes[1].set_title("Correlation of LDLC with Other Variables")
axes[1].set_xlabel("Correlation coefficient (r)")
axes[1].invert_yaxis()

plt.tight_layout()
plt.show()

"""--> Check if the high missing values can be meaningfully imputed. But no, missingness of 74% is too high even with model based imputation and strong correlation with totchol. --> drop HDLC and LDLC"""

#check if totchol is MAR; MNAR OR MCAR (if it correlates with another variable)

#get boolean as 0/1
dhs['TOTCHOL_missing'] = dhs['TOTCHOL'].isna().astype(int)
numeric_vars = ['AGE', 'SYSBP', 'DIABP', 'BMI', 'HEARTRTE']
# concatenation of numeric vars and the flag
corrs = dhs[numeric_vars + ['TOTCHOL_missing']].corr()['TOTCHOL_missing'].sort_values(ascending=False)
print(corrs)

categorical_vars = ['SEX', 'CURSMOKE', 'DIABETES', 'BPMEDS',
                    'PERIOD', 'ANYCHD', 'STROKE', 'CVD', 'HYPERTEN']

for col in categorical_vars:
    rates = dhs.groupby(col)['TOTCHOL_missing'].mean() * 100
    print("\nMissingness rate by", col, ":")
    print(rates.round(2))

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

categorical_vars = ['SEX', 'CURSMOKE', 'DIABETES', 'BPMEDS', 'PERIOD',
                    'ANYCHD', 'STROKE', 'CVD', 'HYPERTEN']


records = []
for col in categorical_vars:
    #calculates percentage of totchol missing per category (eg sex 1 / 2, BPMEDS 0 / 1 etc)
    rates = dhs.groupby(col)['TOTCHOL_missing'].mean() * 100
    #rates is now filled with category and percentage
    for cat, val in rates.items():
        records.append({'Variable': col, 'Category': cat, 'MissingRate': val})

missing_df = pd.DataFrame(records)

# pivot
heatmap_data = missing_df.pivot(index='Variable', columns='Category', values='MissingRate')

# plot
plt.figure(figsize=(10,5))
sns.heatmap(heatmap_data, annot=True, fmt=".1f", cmap='Blues', cbar_kws={'label': 'Missing TOTCHOL (%)'})
plt.title('Missingness of Total Cholesterol (TOTCHOL) by Category', fontsize=14, weight='bold')
plt.xlabel('Category Value')
plt.ylabel('Variable')
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

sns.heatmap(dhs[numeric_vars + ['TOTCHOL_missing']].corr(), annot=True, cmap='coolwarm')
plt.title("Correlation of TOTCHOL Missingness with Numeric Variables")
plt.show()

"""--> Some variables show slight variation in missingness between groups (for example diabetes vs non diabetes, or the different periods where later periods show higher missingness.) This essentially means the missingness for these variables is MAR and should not simply be imputed using mean but rather model-based imputation."""

corr_totchol = dhs.corr(numeric_only=True)['TOTCHOL'].sort_values(ascending=False)
print(corr_totchol)

"""--> From that we have to choose proper predictors.

--> LDLC is too dependent on TOTCHOL and has 70% missing, HDLC has also 70% missing.

--> Randid is not interesting.

--> PERIOD; STROKE; DIABETES; CURSMOKE carry only weak correlations < abs(0.1) and can be discarded.

--> BPMEDS are discarded as well as we may drop them instead of imputing them.
"""

#save current dataset as unimputed dataset to check distribution before and after.
dhs_unimputed = dhs

"""Check distribution of totchol before imputation in detail"""

import seaborn as sns
import matplotlib.pyplot as plt

# statistics summary
print(dhs_unimputed['TOTCHOL'].describe().round(2))

plt.figure(figsize=(8,5))
sns.histplot(dhs_unimputed['TOTCHOL'], bins=30, kde=True, color='teal', alpha=0.6)
plt.axvline(dhs_unimputed['TOTCHOL'].mean(), color='red', linestyle='--', label='Mean: ' + str(round(dhs_unimputed['TOTCHOL'].mean(), 2)))
plt.axvline(dhs_unimputed['TOTCHOL'].median(), color='blue', linestyle='-', label='Median: ' + str(dhs_unimputed['TOTCHOL'].median()))
plt.legend()
plt.title('TOTCHOL Distribution with Mean and Median')
plt.xlabel('Total Cholesterol (mg/dL)')
plt.tight_layout()
plt.show()

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import pandas as pd

predictors = [
    'AGE', 'SEX', 'SYSBP', 'DIABP', 'BMI',
    'HYPERTEN', 'ANYCHD', 'CVD', 'HEARTRTE'
]

cols = predictors + ['TOTCHOL']
X = dhs[cols].copy()

imp = IterativeImputer(random_state=0)
X_imputed = pd.DataFrame(imp.fit_transform(X), columns=cols)

# replace in main dataset
dhs['TOTCHOL'] = X_imputed['TOTCHOL']

import seaborn as sns
import matplotlib.pyplot as plt

# statistics summary
print(dhs['TOTCHOL'].describe().round(2))

plt.figure(figsize=(8,5))
sns.histplot(dhs['TOTCHOL'], bins=30, kde=True, color='teal', alpha=0.6)
plt.axvline(dhs['TOTCHOL'].mean(), color='red', linestyle='--', label='Mean: ' + str(round(dhs['TOTCHOL'].mean(), 2)))
plt.axvline(dhs['TOTCHOL'].median(), color='blue', linestyle='-', label='Median: ' + str(dhs['TOTCHOL'].median()))
plt.legend()
plt.title('TOTCHOL Distribution with Mean and Median')
plt.xlabel('Total Cholesterol (mg/dL)')
plt.tight_layout()
plt.show()

"""--> Imputation did not change much of the distribution so that is good.

--> Continue with BMI and HEARTRTE. For these we choose simple mean imputation.

"""

import seaborn as sns
import matplotlib.pyplot as plt

cols = ['BMI', 'HEARTRTE']

for col in cols:
    print(dhs[col].describe().round(2))

    plt.figure(figsize=(8,5))
    sns.histplot(dhs[col], bins=30, kde=True, color='teal', alpha=0.6)
    plt.axvline(dhs[col].mean(), color='red', linestyle='--', label='Mean: ' + str(round(dhs[col].mean(), 2)))
    plt.axvline(dhs[col].median(), color='blue', linestyle='-', label='Median: ' + str(round(dhs[col].median(), 2)))
    plt.legend()
    plt.title(col + ' Distribution with Mean and Median')
    plt.xlabel(col)
    plt.tight_layout()
    plt.show()

# impute the variables with mean
for col in cols:
    mean_value = dhs[col].mean()
    dhs[col] = dhs[col].fillna(mean_value)

dhs.isna().sum()

import seaborn as sns
import matplotlib.pyplot as plt

cols = ['BMI', 'HEARTRTE']

for col in cols:
    print(dhs[col].describe().round(2))

    plt.figure(figsize=(8,5))
    sns.histplot(dhs[col], bins=30, kde=True, color='orange', alpha=0.6)
    plt.axvline(dhs[col].mean(), color='red', linestyle='--', label='Mean: ' + str(round(dhs[col].mean(), 2)))
    plt.axvline(dhs[col].median(), color='blue', linestyle='-', label='Median: ' + str(round(dhs[col].median(), 2)))
    plt.legend()
    plt.title(col + ' Distribution with Mean and Median')
    plt.xlabel(col)
    plt.tight_layout()
    plt.show()

"""--> Almost nothing changed so thats good. Can keep it as it is.

--> Now finally only drop hdlc and ldlc.
"""

dhs.drop(columns=['LDLC', 'HDLC'], inplace=True)

"""--> and lastly think about how to handle missing bp medication. Just get rid of it??

--> Then scale and encode (especially gender not as 1 2 but something else)

--> Hypertension is chronic. Once patients take BP medication they very rarely if ever stop it. So we could use this domain knowledge and see if at any timepoint (period) they ever took bp. Then we can forwardly impute the missing values.
"""

import numpy as np
import pandas as pd

# show longitudinal data for every patient (randid) with period as columns and bpmeds as cells
bp = (
    dhs
    .pivot_table(index='RANDID', columns='PERIOD', values='BPMEDS', aggfunc='first')
    .rename(columns={1: 'p1', 2: 'p2', 3: 'p3'})
)

# split patients into on meds and never on meds
ever_on_meds = bp.eq(1).any(axis=1)
ever_not_on_meds = bp.eq(0).all(axis=1)

#check if a patient ever goes from 1 to 0 (ignoring nan)
def breaks_rule(row):
    vals = row.tolist()                # convert to list eg [0, 1, 0]
    if 1 not in vals:
        return False                   # never took meds no violation
    first_one = vals.index(1)          # index of first 1
    later = vals[first_one + 1:]       # values after first index
    for v in later:
        if v == 0:                     # found a 0 after the first 1
            return True
    return False                       # otherwise stayed on meds


# call the function
exceptions = bp[ever_on_meds & bp.apply(breaks_rule, axis=1)]

# for ratio
total_ever_on = ever_on_meds.sum()
n_exceptions = len(exceptions)

print("Total patients who ever had BPMEDS = 1:", total_ever_on)
print("Total patients who never took BPMEDS = 1:", ever_not_on_meds.sum())
print("Patients who later had a 0 (stopped medication):", n_exceptions)
print(round(100 * n_exceptions / total_ever_on, 1), "% of ever-med patients later had a 0.\n")

print("Example patients who broke the rule (first 10):")
print(exceptions.head(10))

"""Actually, we can impute more:
Rule	Pattern across periods [p1, p2, p3]	When to apply	Impute
A	[NaN, 0, 0]	Missing at p1, both later periods are 0	p1 â†’ 0
B	[0, NaN, 0]	p1=0, missing at p2, p3=0	p2 â†’ 0
C	[?, 1, NaN]	p2=1, missing at p3	p3 â†’ 1
D	[1, NaN, ?]	p1=1, missing at p2	p2 â†’ 1

Apply it now...
"""

import pandas as pd
import numpy as np

bp = (
    dhs
    .pivot_table(index='RANDID', columns='PERIOD', values='BPMEDS', aggfunc='first')
    .rename(columns={1: 'p1', 2: 'p2', 3: 'p3'})
)

# imputation rules
# Rule D: p1 = 1 and p2 is nan -> p2 = 1
maskD = (bp['p1'] == 1) & (bp['p2'].isna())

# Rule C: p2 = 1 and p3 is nan -> p3 = 1
maskC = (bp['p2'] == 1) & (bp['p3'].isna())

# Rule B: p2 is nan and p3 = 0 -> p2 = 0
maskB = (bp['p2'].isna()) & (bp['p3'] == 0)

# Rule A: p1 is nan and p2 = 0
maskA = bp['p1'].isna() & (bp['p2'] == 0)

# printing some stats
print("Theoretical imputations possible:")
print("Rule D (p1=1 -> p2=1):", maskD.sum())
print("Rule C (p2=1 -> p3=1):", maskC.sum())
print("Rule B ([? ,nan,0] -> p2=0):", maskB.sum())
print("Rule A ([nan,0, ?] -> p1=0):", maskA.sum())
print("Total unique imputations possible:", (maskA | maskB | maskC | maskD).sum())

# set the values of the periods to 1 or 0 according to the masks
bp.loc[maskD, 'p2'] = 1
bp.loc[maskC, 'p3'] = 1
bp.loc[maskB, 'p2'] = 0
bp.loc[maskA, 'p1'] = 0


# revert to original format
bp_long = (
    bp.reset_index()
      .melt(id_vars='RANDID', value_vars=['p1', 'p2', 'p3'],
            var_name='PERIOD', value_name='BPMEDS')
)
bp_long['PERIOD'] = bp_long['PERIOD'].map({'p1': 1, 'p2': 2, 'p3': 3})

# now drop BPMEDS with nans and use BPMEDS from bp_long to replace it
dhs_imputed = (
    dhs.drop(columns=['BPMEDS'])
       .merge(bp_long, on=['RANDID', 'PERIOD'], how='left')
)

# output printing
print("\nImputation applied successfully.")
print("Remaining missing BPMEDS values:", dhs_imputed['BPMEDS'].isna().sum())
print("Example of imputed BPMEDS values:")
print(dhs_imputed[['RANDID', 'PERIOD', 'BPMEDS']].head(10))

dhs_imputed.isna().sum()

# find missing rows after imputation
missing_bpmeds = dhs_imputed[dhs_imputed['BPMEDS'].isna()]

print("Still missing BPMEDS values:", len(missing_bpmeds), "rows (",
      missing_bpmeds['RANDID'].nunique(), "unique patients )\n")

print("Example RANDIDs and periods still missing:")
print(missing_bpmeds[['RANDID', 'PERIOD']].head(20))

# pivot again to print timeline for debugging
bp_missing = (
    dhs_imputed[dhs_imputed['RANDID'].isin(missing_bpmeds['RANDID'])]
    .pivot_table(index='RANDID', columns='PERIOD', values='BPMEDS', aggfunc='first')
    .rename(columns={1:'p1', 2:'p2', 3:'p3'})
)

print("\nExample patients with remaining missing BPMEDS after imputation:")
print(bp_missing.to_string())

"""Drop the remaining whole patients!"""

dhs_imputed = dhs_imputed[~dhs_imputed['RANDID'].isin(bp_missing.index)].copy()

dhs_imputed.isna().sum()

still_missing = dhs_imputed[dhs_imputed['BPMEDS'].isna()]
print(still_missing[['RANDID', 'PERIOD']].to_string(index=False))

dhs_imputed = dhs_imputed[~dhs_imputed['RANDID'].isin(dhs_imputed.loc[dhs_imputed['BPMEDS'].isna(), 'RANDID'])]

dhs_imputed.isna().sum()

dhs_imputed.to_csv('dhs_imputed_clean.csv', index=False)

"""okay theres stuff happening above I have to look into again. but from here we can use dhs_imputed to answer our research questions.





##############################################################################

#RQs

## RQ 3: What lifestyle / health change has best influence on lowering risk for cardiovacular events?
"""

import pandas as pd

# Create a patient-level flag: ever had CVD
patient_cvd = dhs_imputed.groupby('RANDID')['CVD'].max().reset_index()
patient_cvd['ever_CVD'] = patient_cvd['CVD']
patient_cvd.drop(columns='CVD', inplace=True)

# Merge back with baseline (first exam) info
baseline = dhs_imputed.sort_values(['RANDID', 'PERIOD']).groupby('RANDID').first().reset_index()

# Combine
df_patients = baseline.merge(patient_cvd, on='RANDID', how='left')

modifiable = ['CURSMOKE','BMI','SYSBP','DIABP','TOTCHOL','HEARTRTE','DIABETES']

group_means = df_patients.groupby('ever_CVD')[modifiable].mean().T
group_means.columns = ['No CVD', 'Ever CVD']
group_means['Difference'] = group_means['Ever CVD'] - group_means['No CVD']
print(group_means.sort_values('Difference', ascending=False))

import seaborn as sns
import matplotlib.pyplot as plt

group_means_sorted = group_means.sort_values('Difference', ascending=False)

plt.figure(figsize=(8,5))
sns.barplot(x='Difference', y=group_means_sorted.index, data=group_means_sorted, palette='coolwarm')
plt.title('Difference in Health Factors (Ever CVD vs Never CVD)', fontsize=14, weight='bold')
plt.xlabel('Mean Difference (Ever CVD - Never CVD)')
plt.ylabel('Variable')
plt.axvline(0, color='gray', linestyle='--')
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

sns.lineplot(data=dhs_imputed, x='PERIOD', y='SYSBP', hue='CVD', estimator='mean', errorbar=None)
plt.title('Mean Systolic BP over Time by CVD Status')
plt.ylabel('Mean SYSBP')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

sns.lineplot(data=dhs_imputed, x='PERIOD', y='TOTCHOL', hue='CVD', estimator='mean', errorbar=None)
plt.title('Mean Systolic BP over Time by CVD Status')
plt.ylabel('Mean SYSBP')
plt.show()

print("Unique patients with CVD = 1:", dhs_imputed.loc[dhs_imputed['CVD'] == 1, 'RANDID'].nunique())

print("Unique patients without CVD = 0:", dhs_imputed.loc[dhs_imputed['CVD'] == 0, 'RANDID'].nunique())

"""## RQ 1: Among participants at baseline, how do demographics and clinical characteristics differ between those who eventually took antihypertensive medication at any of the three follow-ups and those who did not?

"""

import pandas as pd
dhs_RQa = dhs_imputed.copy()
dhs_RQa

import pandas as pd
import numpy as np

#copy dataset
#HAVE TO CHANGE IT TO THE IMPUTED ONE ONCE IT IS IMPUTED
#dhs_RQa = dhs.copy()

#identify patients who ever took antihypertensive meds
ever_users = dhs_RQa.groupby('RANDID')['BPMEDS'].max()  # 1 if ever used, 0 if never
ever_users = ever_users.reset_index().rename(columns={'BPMEDS':'ever_BPMEDS'})

#extract baseline characteristics (period 1)
baseline = dhs_RQa[dhs_RQa['PERIOD'] == 1].copy()

# Merge baseline with ever-users
baseline = baseline.merge(ever_users, on='RANDID', how='left')

#separate numeric and categorical variables
continuous_vars = ['AGE', 'BMI', 'TOTCHOL', 'SYSBP', 'DIABP', 'HEARTRTE'] #'HDLC', 'LDLC'
categorical_vars = ['SEX', 'CURSMOKE', 'DIABETES', 'BPMEDS',
                    'ANYCHD', 'STROKE', 'CVD', 'HYPERTEN']

#summarize for continuous variables

print("Continuous Variables by Ever Use of Antihypertensive Medication")

cont_summary_dict = {}

for var in continuous_vars:
    # summary stats per group
    summary = (baseline.groupby('ever_BPMEDS')[var]
        .agg(['mean', 'std', 'median', 'min', 'max', 'count'])
        .round(2))
    summary.index = ['Never Used Meds', 'Ever Used Meds']
    cont_summary_dict[var] = summary

    print(f"\n{var} by Ever Use of Antihypertensive Medication:\n")
    print(summary.style.set_caption(f"{var}")
        .set_table_styles([
            {'selector': 'caption',
             'props': [('font-size', '15px'),
                       ('text-align', 'center'),
                       ('font-weight', 'bold')]}])
        .set_properties(**{'text-align': 'center'})
        #.background_gradient(cmap="Blues", axis=None)
    )


#summarize for categorical variables

print("\nCategorical Variables by Ever Use of Antihypertensive Medication")

cat_tables = {}
for var in categorical_vars:
    ct = pd.crosstab(baseline[var], baseline['ever_BPMEDS'])
    ct_percent = (ct.div(ct.sum(axis=0), axis=1) * 100).round(1)
    combined = ct.astype(str) + " (" + ct_percent.astype(str) + "%)"
    combined = combined.T
    combined.columns.name = var
    combined.index = ['Never Used Meds', 'Ever Used Meds']
    cat_tables[var] = combined

    print(f"\n{var} by Ever Use of Antihypertensive Medication:\n")
    print(combined.style.set_caption(f"{var}: Counts (Percentages)")
        .set_table_styles([
            {'selector': 'caption',
             'props': [('font-size', '14px'),
                       ('text-align', 'center'),
                       ('font-weight', 'bold')]}])
        .set_properties(**{'text-align': 'center'}))

#show distributions for continuous variables
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

colors = {0: "skyblue", 1: "salmon"}

#continuous variables sistributions
print("Distributions of Continuous Variables by Ever Use of Antihypertensive Medication")

for var in continuous_vars:
    fig, ax = plt.subplots(figsize=(6, 5))

    #separate data by meds use
    group0 = baseline.loc[baseline['ever_BPMEDS'] == 0, var]
    group1 = baseline.loc[baseline['ever_BPMEDS'] == 1, var]

    # calculate means
    mean0 = group0.mean()
    mean1 = group1.mean()

    #show overlapping histograms
    ax.hist(group0, bins=30, alpha=0.6, label='Never Used Meds', color=colors[0], edgecolor='black')
    ax.hist(group1, bins=30, alpha=0.6, label='Ever Used Meds', color=colors[1], edgecolor='black')

    # Add mean lines
    ax.axvline(mean0, color=colors[0], linestyle='--', linewidth=2)
    ax.axvline(mean1, color=colors[1], linestyle='--', linewidth=2)

    # Annotate means
    #ax.text(mean0, ax.get_ylim()[1]*0.9, f"Mean={mean0:.2f}", color=colors[0], rotation=90, ha='right', fontsize=10)
    #ax.text(mean1, ax.get_ylim()[1]*0.9, f"Mean={mean1:.2f}", color=colors[1], rotation=90, ha='left', fontsize=10)

    ax.set_title(f"{var} Distribution by Medication Use", fontsize=14, weight='bold')
    ax.set_xlabel(var)
    ax.set_ylabel("Count")
    ax.legend()
    plt.tight_layout()
    plt.show()


#show bar plots for categorical variables
print("Categorical Variable Distributions by Ever Use of Antihypertensive Medication")

for var in categorical_vars:
    fig, ax = plt.subplots(figsize=(6, 5))
    cat_table = pd.crosstab(baseline['ever_BPMEDS'], baseline[var], normalize='index') * 100
    cat_table.T.plot(kind='bar', ax=ax, color=['skyblue', 'salmon'])

    ax.set_title(f"{var} by medication use", weight='bold')
    ax.set_xlabel(var)
    ax.set_ylabel("Percentage (%)")
    ax.legend(["Never Used Meds", "Ever Used Meds"], title="Medication Use")
    plt.xticks(rotation=0)
    plt.tight_layout()
    plt.show()

    #median & mean



"""## RQ 2: How does the occurrence of major cardiovascular events differ between non-hypertensive participants and hypertensive participants, and among the latter, between those treated with antihypertensive medication and those untreated across the three follow-up periods?

"""

import pandas as pd

#copy dataset (replace with imputed one once ready)
dhs_RQb = dhs_RQa.copy()

events = ['STROKE', 'ANYCHD', 'CVD']

#sort by patient and period
dhs_RQb_sorted = dhs_RQb.sort_values(['RANDID', 'PERIOD']).copy()

patient_summary = []

#go through grouped table and take unique ids and groups of rows for each unique id
for pid, group in dhs_RQb_sorted.groupby('RANDID'):
    patient_row = {'RANDID': pid}

    #define hypertension status
    #ever hypertensive during follow-up (HYPERTEN = 1)
    hypertensive = group['HYPERTEN'].max() == 1 #for this code assume that per patient per period it is different, but actually it is the same
    patient_row['HYPERTEN'] = int(hypertensive)

    #ever on antihypertensive medication at any period
    on_meds = group['BPMEDS'].max() == 1
    patient_row['EVER_BPMEDS'] = int(on_meds)

    #event occurrence (ever happened during follow-up)
    for event in events:
        patient_row[event] = group[event].max() #for this code assume that per patient per period it is different, but actually it is the same

    patient_summary.append(patient_row)

#create summary DataFrame
dhs_RQb_summary = pd.DataFrame(patient_summary)

#define and name subgroups for clarity
def classify_patient(row):
    if row['HYPERTEN'] == 0:
        return 'Non-hypertensive'
    elif row['HYPERTEN'] == 1 and row['EVER_BPMEDS'] == 1:
        return 'Hypertensive, treated'
    else:
        return 'Hypertensive, untreated'

dhs_RQb_summary['GROUP'] = dhs_RQb_summary.apply(classify_patient, axis=1)

#write a function to show counts and percentages per group for each event
def event_summary(df, event):
    ctab = pd.crosstab(df['GROUP'], df[event])
    ctab_percent = ctab.div(ctab.sum(axis=1), axis=0) * 100 #do it in proportions bc sample sizes are different --> but still report sizes in the presentation
    combined = ctab.astype(str) + " (" + ctab_percent.round(1).astype(str) + "%)"
    combined.columns = ['No event', 'Event']
    return combined

#show summaries
for event in events:
    print(f"\nOccurrence of {event} by hypertension and medication status")
    print(event_summary(dhs_RQb_summary, event))

import matplotlib.pyplot as plt

plot_data = []

for event in events:
    ctab = pd.crosstab(dhs_RQb_summary['GROUP'], dhs_RQb_summary[event])
    ctab_percent = ctab.div(ctab.sum(axis=1), axis=0) * 100
    event_df = ctab_percent.reset_index()
    event_df['Event'] = event
    plot_data.append(event_df[['GROUP', 1, 'Event']])  # take only the % of those who had the event

#combine all events into one dataframe
plot_df = pd.concat(plot_data).rename(columns={1: 'Event Rate (%)'})

#plot each event as a separate barplot
fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)

for ax, event in zip(axes, events):
    subset = plot_df[plot_df['Event'] == event]
    ax.bar(subset['GROUP'], subset['Event Rate (%)'], color=['#80b1d3', '#fdb462', '#b3de69'])
    ax.set_title(event, fontsize=12, weight='bold')
    ax.set_xlabel('Group')
    ax.set_ylim(0, 100)
    ax.grid(axis='y', linestyle='--', alpha=0.7)
    ax.set_xticklabels(subset['GROUP'], rotation=15, ha='right')
    if ax == axes[0]:
        ax.set_ylabel('Event occurrence (%)')

plt.suptitle('Occurrence of Cardiovascular Events by Hypertension and Medication Status', fontsize=14, weight='bold')
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()





